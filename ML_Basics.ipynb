{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkv/kV1jWtEKgJlmvS1OKH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dopey-tim/Bus4-118S/blob/main/ML_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lU6Wd0wHnmuu",
        "outputId": "ef04986f-f522-4f50-e226-d3763904d494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted price for a 2000 sq ft house in Downtown: $739,790.61\n",
            "\n",
            "Model Coefficients:\n",
            "location_Downtown: 303119.17\n",
            "location_Rural: -260346.04\n",
            "location_Suburb: -42773.12\n",
            "square_footage: 209.35\n",
            "Intercept: 17,970.39\n",
            "\n",
            "Model RÂ² Score on test data: 0.92\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Generate synthetic dataset with 150 entries\n",
        "np.random.seed(42)\n",
        "n_samples = 150\n",
        "\n",
        "# Square footage between 800 and 4000\n",
        "square_footage = np.random.randint(800, 4000, n_samples)\n",
        "\n",
        "# Randomly assign locations\n",
        "locations = np.random.choice(['Downtown', 'Suburb', 'Rural'], n_samples)\n",
        "\n",
        "# Base price per sq ft by location\n",
        "price_per_sqft = {\n",
        "    'Downtown': 350,\n",
        "    'Suburb': 200,\n",
        "    'Rural': 120\n",
        "}\n",
        "\n",
        "# Generate prices with noise\n",
        "prices = [\n",
        "    sqft * price_per_sqft[loc] + np.random.randint(-20000, 20000)\n",
        "    for sqft, loc in zip(square_footage, locations)\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'square_footage': square_footage,\n",
        "    'location': locations,\n",
        "    'price': prices\n",
        "})\n",
        "\n",
        "# Features and target\n",
        "X = df[['square_footage', 'location']]\n",
        "y = df['price']\n",
        "\n",
        "# Preprocessing: One-hot encode location\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('location', OneHotEncoder(sparse_output=False), ['location'])\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Pipeline with preprocessing + regression\n",
        "model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make prediction for a new house: 2000 sq ft in Downtown\n",
        "new_house = pd.DataFrame({'square_footage': [2000], 'location': ['Downtown']})\n",
        "predicted_price = model.predict(new_house)[0]\n",
        "print(f\"Predicted price for a 2000 sq ft house in Downtown: ${predicted_price:,.2f}\")\n",
        "\n",
        "# Display model coefficients\n",
        "feature_names = (\n",
        "    model.named_steps['preprocessor']\n",
        "    .named_transformers_['location']\n",
        "    .get_feature_names_out(['location'])\n",
        ").tolist() + ['square_footage']\n",
        "\n",
        "coefficients = model.named_steps['regressor'].coef_\n",
        "intercept = model.named_steps['regressor'].intercept_\n",
        "\n",
        "print(\"\\nModel Coefficients:\")\n",
        "for feature, coef in zip(feature_names, coefficients):\n",
        "    print(f\"{feature}: {coef:.2f}\")\n",
        "print(f\"Intercept: {intercept:,.2f}\")\n",
        "\n",
        "# Model performance\n",
        "r2_score = model.score(X_test, y_test)\n",
        "print(f\"\\nModel RÂ² Score on test data: {r2_score:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Synthetic churn dataset with 150 entries\n",
        "# Inspired by Kaggle Telco Customer Churn dataset\n",
        "# https://www.kaggle.com/blastchar/telco-customer-churn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "np.random.seed(42)\n",
        "n_samples = 150\n",
        "\n",
        "# Generate synthetic features\n",
        "age = np.random.randint(20, 70, n_samples)\n",
        "monthly_usage_hours = np.random.randint(5, 100, n_samples)\n",
        "purchase_amount = np.random.randint(50, 1000, n_samples)\n",
        "customer_service_calls = np.random.randint(0, 10, n_samples)\n",
        "regions = np.random.choice(['North', 'South', 'East', 'West'], n_samples)\n",
        "\n",
        "# Simulate churn with some logic\n",
        "# High churn chance if low usage + high service calls + low purchases\n",
        "churn = [\n",
        "    1 if (usage < 20 and calls > 5) or (spend < 200) else 0\n",
        "    for usage, calls, spend in zip(monthly_usage_hours, customer_service_calls, purchase_amount)\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'age': age,\n",
        "    'monthly_usage_hours': monthly_usage_hours,\n",
        "    'purchase_amount': purchase_amount,\n",
        "    'customer_service_calls': customer_service_calls,\n",
        "    'region': regions,\n",
        "    'churn': churn\n",
        "})\n",
        "\n",
        "# Features and target\n",
        "X = df.drop('churn', axis=1)\n",
        "y = df['churn']\n",
        "\n",
        "# Preprocessing: scale numerical + one-hot encode categorical\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), ['age', 'monthly_usage_hours', 'purchase_amount', 'customer_service_calls']),\n",
        "        ('cat', OneHotEncoder(sparse_output=False), ['region'])\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Pipeline with logistic regression\n",
        "model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
        "])\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict churn probability for a new customer\n",
        "new_customer = pd.DataFrame({\n",
        "    'age': [40],\n",
        "    'monthly_usage_hours': [25],\n",
        "    'purchase_amount': [300],\n",
        "    'customer_service_calls': [4],\n",
        "    'region': ['West']\n",
        "})\n",
        "churn_probability = model.predict_proba(new_customer)[0][1]\n",
        "churn_prediction = int(churn_probability > 0.5)\n",
        "\n",
        "print(f\"Churn Probability for new customer: {churn_probability:.2f}\")\n",
        "print(f\"Churn Prediction (1 = churn, 0 = no churn): {churn_prediction}\")\n",
        "\n",
        "# Model coefficients\n",
        "feature_names = (\n",
        "    model.named_steps['preprocessor']\n",
        "    .get_feature_names_out()\n",
        ")\n",
        "coefficients = model.named_steps['classifier'].coef_[0]\n",
        "intercept = model.named_steps['classifier'].intercept_[0]\n",
        "\n",
        "print(\"\\nModel Coefficients:\")\n",
        "for feature, coef in zip(feature_names, coefficients):\n",
        "    print(f\"{feature}: {coef:.2f}\")\n",
        "print(f\"Intercept: {intercept:.2f}\")\n",
        "\n",
        "# Model performance\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "roc_score = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
        "print(f\"ROC-AUC Score: {roc_score:.2f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSqWm-BVpysL",
        "outputId": "3d57d8ad-c76c-456a-babb-1b36d992b104"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Churn Probability for new customer: 0.30\n",
            "Churn Prediction (1 = churn, 0 = no churn): 0\n",
            "\n",
            "Model Coefficients:\n",
            "num__age: -0.27\n",
            "num__monthly_usage_hours: -0.58\n",
            "num__purchase_amount: -1.67\n",
            "num__customer_service_calls: 0.41\n",
            "cat__region_East: 0.32\n",
            "cat__region_North: 0.49\n",
            "cat__region_South: -0.72\n",
            "cat__region_West: -0.09\n",
            "Intercept: -2.73\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      1.00      0.92        22\n",
            "           1       1.00      0.50      0.67         8\n",
            "\n",
            "    accuracy                           0.87        30\n",
            "   macro avg       0.92      0.75      0.79        30\n",
            "weighted avg       0.89      0.87      0.85        30\n",
            "\n",
            "Confusion Matrix:\n",
            "[[22  0]\n",
            " [ 4  4]]\n",
            "ROC-AUC Score: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Synthetic customer segmentation dataset with 150 entries\n",
        "# Inspired by UCI Online Retail dataset\n",
        "# https://archive.ics.uci.edu/ml/datasets/online+retail\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic dataset\n",
        "np.random.seed(42)\n",
        "n_samples = 150\n",
        "\n",
        "annual_spending = np.random.randint(500, 20000, n_samples)\n",
        "purchase_frequency = np.random.randint(1, 50, n_samples)\n",
        "age = np.random.randint(20, 70, n_samples)\n",
        "region = np.random.choice(['North', 'South', 'East', 'West'], n_samples)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'annual_spending': annual_spending,\n",
        "    'purchase_frequency': purchase_frequency,\n",
        "    'age': age,\n",
        "    'region': region\n",
        "})\n",
        "\n",
        "# Select numerical features and scale\n",
        "features = ['annual_spending', 'purchase_frequency', 'age']\n",
        "X = df[features]\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Elbow method to find optimal clusters\n",
        "inertia = []\n",
        "K = range(1, 8)\n",
        "for k in K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(K, inertia, 'bo-')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Optimal K')\n",
        "plt.savefig('elbow_plot.png')\n",
        "plt.close()\n",
        "\n",
        "# Apply KMeans with chosen K (e.g., 3)\n",
        "optimal_k = 3\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "df['cluster'] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Evaluate clustering\n",
        "sil_score = silhouette_score(X_scaled, df['cluster'])\n",
        "print(f\"Silhouette Score: {sil_score:.2f}\")\n",
        "\n",
        "# Cluster summary\n",
        "cluster_summary = df.groupby('cluster')[features].mean().round(2)\n",
        "cluster_counts = df['cluster'].value_counts()\n",
        "\n",
        "print(\"\\nCluster Characteristics:\")\n",
        "print(cluster_summary)\n",
        "print(\"\\nCluster Counts:\")\n",
        "print(cluster_counts)\n",
        "\n",
        "# Example targeted strategies\n",
        "for cluster in range(optimal_k):\n",
        "    print(f\"\\nCluster {cluster} Strategy:\")\n",
        "    if cluster_summary.loc[cluster, 'annual_spending'] > 12000:\n",
        "        print(\"ðŸ’Ž High-spending customers: Offer VIP programs, loyalty rewards, and exclusive promotions.\")\n",
        "    elif cluster_summary.loc[cluster, 'purchase_frequency'] > 25:\n",
        "        print(\"ðŸ“¦ Frequent buyers: Provide subscription services, bulk discounts, or early access to products.\")\n",
        "    else:\n",
        "        print(\"ðŸ“¢ Low-engagement customers: Send personalized re-engagement campaigns and starter offers.\")\n",
        "\n",
        "# Save datasets\n",
        "df.to_csv('customer_segments.csv', index=False)\n",
        "cluster_summary.to_csv('cluster_summary.csv')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GACTT02HtAPV",
        "outputId": "22e0c989-63bc-4315-a263-d4c006066bdc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Score: 0.26\n",
            "\n",
            "Cluster Characteristics:\n",
            "         annual_spending  purchase_frequency    age\n",
            "cluster                                            \n",
            "0                4418.74               15.26  52.26\n",
            "1               13244.22               18.08  34.78\n",
            "2               11251.97               39.31  51.40\n",
            "\n",
            "Cluster Counts:\n",
            "cluster\n",
            "2    58\n",
            "1    49\n",
            "0    43\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Cluster 0 Strategy:\n",
            "ðŸ“¢ Low-engagement customers: Send personalized re-engagement campaigns and starter offers.\n",
            "\n",
            "Cluster 1 Strategy:\n",
            "ðŸ’Ž High-spending customers: Offer VIP programs, loyalty rewards, and exclusive promotions.\n",
            "\n",
            "Cluster 2 Strategy:\n",
            "ðŸ“¦ Frequent buyers: Provide subscription services, bulk discounts, or early access to products.\n"
          ]
        }
      ]
    }
  ]
}