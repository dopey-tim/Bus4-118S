{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+N/+og4rOg00mTjLlaBlF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dopey-tim/Bus4-118S/blob/main/ML_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lU6Wd0wHnmuu",
        "outputId": "7f730194-f4b2-4693-a22f-fdf7c29dbafc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted price for a 2000 sq ft house in Downtown: $568,841.40\n",
            "\n",
            "Model Coefficients:\n",
            "location_Downtown: 153,216.71\n",
            "location_Rural: -133,456.25\n",
            "location_Suburb: -19,760.46\n",
            "square_footage: 121.75\n",
            "\n",
            "Intercept (baseline price): $172,130.04\n",
            "\n",
            "Model R² Score on test data: 0.91\n"
          ]
        }
      ],
      "source": [
        "# Source: Approximate values inspired by public housing market data from Zillow Research\n",
        "# (https://www.zillow.com/research/data/) – simplified for demonstration purposes.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Generate more realistic sample data (prices in USD, square footage, locations)\n",
        "data = {\n",
        "    'square_footage': [1200, 1500, 2000, 2200, 1800, 2500, 3000, 3500, 4000, 2800],\n",
        "    'location': ['Downtown', 'Suburb', 'Downtown', 'Rural', 'Suburb',\n",
        "                 'Downtown', 'Rural', 'Suburb', 'Downtown', 'Rural'],\n",
        "    'price': [350000, 420000, 600000, 320000, 450000,\n",
        "              720000, 400000, 500000, 850000, 370000]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Features and target\n",
        "X = df[['square_footage', 'location']]\n",
        "y = df['price']\n",
        "\n",
        "# Preprocessing: One-hot encode the location column\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('location', OneHotEncoder(sparse_output=False), ['location'])\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Create pipeline with preprocessing and model\n",
        "model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make prediction for a new house: 2000 sq ft in Downtown\n",
        "new_house = pd.DataFrame({'square_footage': [2000], 'location': ['Downtown']})\n",
        "predicted_price = model.predict(new_house)[0]\n",
        "\n",
        "print(f\"Predicted price for a 2000 sq ft house in Downtown: ${predicted_price:,.2f}\")\n",
        "\n",
        "# Display model coefficients\n",
        "feature_names = (\n",
        "    model.named_steps['preprocessor']\n",
        "    .named_transformers_['location']\n",
        "    .get_feature_names_out(['location'])\n",
        ").tolist() + ['square_footage']\n",
        "\n",
        "coefficients = model.named_steps['regressor'].coef_\n",
        "intercept = model.named_steps['regressor'].intercept_\n",
        "\n",
        "print(\"\\nModel Coefficients:\")\n",
        "for feature, coef in zip(feature_names, coefficients):\n",
        "    print(f\"{feature}: {coef:,.2f}\")\n",
        "\n",
        "print(f\"\\nIntercept (baseline price): ${intercept:,.2f}\")\n",
        "\n",
        "# Model performance\n",
        "r2_score = model.score(X_test, y_test)\n",
        "print(f\"\\nModel R² Score on test data: {r2_score:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Source: Values inspired by the public \"Telco Customer Churn\" dataset on Kaggle\n",
        "# https://www.kaggle.com/blastchar/telco-customer-churn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "# Sample synthetic customer churn data\n",
        "data = {\n",
        "    'age': [25, 42, 51, 30, 60, 36, 48, 29, 55, 33],\n",
        "    'monthly_usage_hours': [15, 80, 25, 10, 95, 45, 30, 12, 70, 50],\n",
        "    'purchase_amount': [120, 400, 180, 90, 500, 250, 200, 100, 420, 300],\n",
        "    'customer_service_calls': [3, 1, 7, 5, 0, 2, 6, 4, 1, 3],\n",
        "    'region': ['North', 'South', 'West', 'East', 'South',\n",
        "               'North', 'West', 'East', 'South', 'North'],\n",
        "    'churn': [1, 0, 1, 1, 0, 0, 1, 1, 0, 0]  # 1 = churned, 0 = not churned\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Features and target\n",
        "X = df.drop('churn', axis=1)\n",
        "y = df['churn']\n",
        "\n",
        "# Preprocessing: scale numerical + one-hot encode categorical\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), make_column_selector(dtype_include=np.number)),\n",
        "        ('cat', OneHotEncoder(sparse_output=False), make_column_selector(dtype_include=object))\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Pipeline with logistic regression\n",
        "model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
        "])\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict churn probability for a new customer\n",
        "new_customer = pd.DataFrame({\n",
        "    'age': [40],\n",
        "    'monthly_usage_hours': [35],\n",
        "    'purchase_amount': [250],\n",
        "    'customer_service_calls': [2],\n",
        "    'region': ['West']\n",
        "})\n",
        "churn_probability = model.predict_proba(new_customer)[0][1]\n",
        "churn_prediction = int(churn_probability > 0.5)\n",
        "\n",
        "print(f\"Churn Probability for new customer: {churn_probability:.2f}\")\n",
        "print(f\"Churn Prediction (1 = churn, 0 = no churn): {churn_prediction}\")\n",
        "\n",
        "# Model coefficients\n",
        "feature_names = (\n",
        "    model.named_steps['preprocessor']\n",
        "    .get_feature_names_out()\n",
        ")\n",
        "coefficients = model.named_steps['classifier'].coef_[0]\n",
        "intercept = model.named_steps['classifier'].intercept_[0]\n",
        "\n",
        "print(\"\\nModel Coefficients:\")\n",
        "for feature, coef in zip(feature_names, coefficients):\n",
        "    print(f\"{feature}: {coef:.2f}\")\n",
        "print(f\"Intercept: {intercept:.2f}\")\n",
        "\n",
        "# Model performance\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "roc_score = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
        "print(f\"ROC-AUC Score: {roc_score:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSqWm-BVpysL",
        "outputId": "d1d24e6f-4393-4830-d849-36116d8b836c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Churn Probability for new customer: 0.53\n",
            "Churn Prediction (1 = churn, 0 = no churn): 1\n",
            "\n",
            "Model Coefficients:\n",
            "num__age: -0.03\n",
            "num__monthly_usage_hours: -0.73\n",
            "num__purchase_amount: -0.71\n",
            "num__customer_service_calls: 0.66\n",
            "cat__region_East: 0.17\n",
            "cat__region_North: -0.38\n",
            "cat__region_South: -0.02\n",
            "cat__region_West: 0.23\n",
            "Intercept: 0.62\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           1.00         2\n",
            "   macro avg       1.00      1.00      1.00         2\n",
            "weighted avg       1.00      1.00      1.00         2\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2]]\n",
            "ROC-AUC Score: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Source: Synthetic values inspired by UCI Online Retail Dataset\n",
        "# (https://archive.ics.uci.edu/ml/datasets/online+retail)\n",
        "# Used for demonstration of customer segmentation.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate more realistic customer data\n",
        "data = {\n",
        "    'annual_spending': [500, 3000, 15000, 700, 12000, 4500, 2000, 8000, 1000, 6000,\n",
        "                        9500, 2500, 400, 11000, 5200],\n",
        "    'purchase_frequency': [5, 18, 45, 3, 40, 22, 12, 35, 6, 28,\n",
        "                           30, 15, 4, 42, 20],\n",
        "    'age': [25, 34, 52, 28, 48, 36, 41, 29, 47, 33,\n",
        "            55, 39, 26, 50, 44],\n",
        "    'region': ['North', 'South', 'West', 'East', 'South',\n",
        "               'North', 'West', 'East', 'South', 'North',\n",
        "               'East', 'West', 'South', 'North', 'East']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Select and scale numerical features\n",
        "features = ['annual_spending', 'purchase_frequency', 'age']\n",
        "X = df[features]\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Determine optimal number of clusters using elbow method\n",
        "inertia = []\n",
        "K = range(1, 6)\n",
        "for k in K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plot elbow curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(K, inertia, 'bo-')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Optimal K')\n",
        "plt.savefig('elbow_plot.png')\n",
        "plt.close()\n",
        "\n",
        "# Apply K-Means with chosen K (assume 3 based on elbow)\n",
        "optimal_k = 3\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "df['cluster'] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Evaluate clustering with silhouette score\n",
        "sil_score = silhouette_score(X_scaled, df['cluster'])\n",
        "print(f\"Silhouette Score: {sil_score:.2f}\")\n",
        "\n",
        "# Analyze clusters\n",
        "cluster_summary = df.groupby('cluster')[features].mean().round(2)\n",
        "cluster_counts = df['cluster'].value_counts()\n",
        "\n",
        "print(\"\\nCluster Characteristics:\")\n",
        "print(cluster_summary)\n",
        "print(\"\\nCluster Counts:\")\n",
        "print(cluster_counts)\n",
        "\n",
        "# Example of targeted strategies\n",
        "for cluster in range(optimal_k):\n",
        "    print(f\"\\nCluster {cluster} Strategy:\")\n",
        "    if cluster_summary.loc[cluster, 'annual_spending'] > 8000:\n",
        "        print(\"💎 High-spending customers: Offer VIP perks, loyalty rewards, and exclusive promotions.\")\n",
        "    elif cluster_summary.loc[cluster, 'purchase_frequency'] > 20:\n",
        "        print(\"📦 Frequent buyers: Provide bulk discounts or subscription plans.\")\n",
        "    else:\n",
        "        print(\"📢 Low-engagement customers: Send personalized re-engagement campaigns or starter offers.\")\n",
        "\n",
        "# Save results\n",
        "df.to_csv('customer_segments.csv', index=False)\n",
        "cluster_summary.to_csv('cluster_summary.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GACTT02HtAPV",
        "outputId": "9ceb4df5-a2e8-4e30-f4a6-cfca6dfff7f4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Score: 0.52\n",
            "\n",
            "Cluster Characteristics:\n",
            "         annual_spending  purchase_frequency    age\n",
            "cluster                                            \n",
            "0                4025.00               19.50  37.88\n",
            "1               11875.00               39.25  51.25\n",
            "2                 533.33                4.00  26.33\n",
            "\n",
            "Cluster Counts:\n",
            "cluster\n",
            "0    8\n",
            "1    4\n",
            "2    3\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Cluster 0 Strategy:\n",
            "📢 Low-engagement customers: Send personalized re-engagement campaigns or starter offers.\n",
            "\n",
            "Cluster 1 Strategy:\n",
            "💎 High-spending customers: Offer VIP perks, loyalty rewards, and exclusive promotions.\n",
            "\n",
            "Cluster 2 Strategy:\n",
            "📢 Low-engagement customers: Send personalized re-engagement campaigns or starter offers.\n"
          ]
        }
      ]
    }
  ]
}