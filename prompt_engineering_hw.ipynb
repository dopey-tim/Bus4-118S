{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dopey-tim/Bus4-118S/blob/main/prompt_engineering_hw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0032144",
      "metadata": {
        "id": "f0032144"
      },
      "source": [
        "\n",
        "# AI Prompt Engineering Homework — Python Implementations\n",
        "\n",
        "This notebook contains working Python examples for three prompt-engineering tasks:\n",
        "\n",
        "1. **Prompt Chaining for a Customer Support AI** — a simple, rule-based chain that simulates a customer service flow and demonstrates how you'd wrap prompts if you were calling an LLM.\n",
        "2. **Code Generation with ReACT Prompting** — a lightweight, illustrative ReACT-style loop that \"reasons\" (as printed logs), writes code, executes tests, and iterates if needed.\n",
        "3. **Self-Reflection Prompt for Improving Output** — a reflective pass that critiques a draft summary and produces an improved version.\n",
        "\n",
        "> **Note:** This notebook uses **no external APIs** by default. Where an LLM call would normally go, you'll find a `call_llm(...)` placeholder and a tiny `simulate_llm(...)` helper so the code runs out-of-the-box. If you want live LLM outputs, implement `call_llm(...)` with your preferred provider.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) Make sure the SDK is current\n",
        "!pip install -q --upgrade openai\n",
        "\n",
        "# 1) Load key from Colab secrets or env\n",
        "import os\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    key = userdata.get(\"Tims-key\")\n",
        "    if key:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = key\n",
        "except Exception:\n",
        "    pass\n",
        "assert os.getenv(\"OPENAI_API_KEY\"), \"Missing OPENAI_API_KEY (or Colab secret 'Tims-key').\"\n",
        "\n",
        "# 2) Use Chat Completions\n",
        "from typing import Optional\n",
        "from openai import OpenAI\n",
        "\n",
        "def call_llm(prompt: str, system: Optional[str] = None) -> str:\n",
        "    client = OpenAI()  # reads OPENAI_API_KEY\n",
        "    messages = []\n",
        "    if system:\n",
        "        messages.append({\"role\": \"system\", \"content\": system})\n",
        "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",   # change to a model you have access to if needed\n",
        "        messages=messages,\n",
        "        temperature=0.2,\n",
        "    )\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "# 3) Quick test\n",
        "print(call_llm(\"Say 'pong' once.\", system=\"You are terse.\"))\n"
      ],
      "metadata": {
        "id": "q-cAG46cZt81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9790627-d828-4756-9284-2fa93f511cd0"
      },
      "id": "q-cAG46cZt81",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pong.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "b9a75d24",
      "metadata": {
        "id": "b9a75d24"
      },
      "outputs": [],
      "source": [
        "from textwrap import dedent\n",
        "import re\n",
        "import traceback\n",
        "from typing import Optional\n",
        "\n",
        "# The call_llm function is now implemented in cell q-cAG46cZt81\n",
        "# def call_llm(prompt: str, system: Optional[str] = None) -> str:\n",
        "#     \"\"\"\n",
        "#     Placeholder for a real LLM call.\n",
        "#     Implement with your provider of choice (e.g., OpenAI, Azure, etc.).\n",
        "#     \"\"\"\n",
        "#     raise NotImplementedError(\"Plug your LLM API here if desired.\")\n",
        "\n",
        "def simulate_llm(prompt: str) -> str:\n",
        "    lower = prompt.lower()\n",
        "\n",
        "    # ... your other branches (intent/entity/next-step) ...\n",
        "\n",
        "    # ReACT-ish code generation guidance (ensure this exact trigger)\n",
        "    if \"react\" in lower and \"write python\" in lower and \"two_sum\" in lower:\n",
        "        return dedent(\"\"\"\n",
        "        Thought: Use a hash map to find complements in O(n).\n",
        "        Action: write_code\n",
        "        Action Input:\n",
        "        ```python\n",
        "        def two_sum(nums, target):\n",
        "            seen = {}\n",
        "            for i, x in enumerate(nums):\n",
        "                comp = target - x\n",
        "                if comp in seen:\n",
        "                    return [seen[comp], i]\n",
        "                seen[x] = i\n",
        "            return None\n",
        "        ```\n",
        "        \"\"\")\n",
        "    # ⬇️ DEFAULT FALLBACK to avoid returning None\n",
        "    return \"Thought: Falling back.\\nAction: write_code\\nAction Input:\\n```python\\n# no-op\\n```\\n\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e06b32e9",
      "metadata": {
        "id": "e06b32e9"
      },
      "source": [
        "\n",
        "## 1) Prompt Chaining for a Customer Support AI\n",
        "\n",
        "This example builds a simple multi-step chain:\n",
        "1. **Intent Classification** → billing / account / technical / other  \n",
        "2. **Entity Extraction** → order IDs, error codes, product names (toy)  \n",
        "3. **Propose Next Step** → a short, helpful action suggestion  \n",
        "4. **Final Response** → a customer-facing message using the chain outputs\n",
        "\n",
        "You can replace `simulate_llm(...)` with `call_llm(...)` to swap in a real model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "35553868",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35553868",
        "outputId": "ed941da6-0f96-4bfa-be94-d869ce9a6130"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "USER: I was double-charged on my invoice for the Pro Plan. Order #12345.\n",
            "INTENT: billing\n",
            "ENTITIES: {}\n",
            "NEXT STEP: Ask the customer for specific details about their billing inquiry, such as the type of billing issue they are experiencing or any relevant account information.\n",
            "FINAL RESPONSE:\n",
            " Thanks for reaching out! I see this looks like a **billing** request.\n",
            "\n",
            "\n",
            "**Next step:** Ask the customer for specific details about their billing inquiry, such as the type of billing issue they are experiencing or any relevant account information.\n",
            "\n",
            "If that sounds good, reply **YES** to proceed, or tell me what to adjust.\n",
            "---\n",
            "USER: I can't log in—password reset keeps failing for my Premium subscription.\n",
            "INTENT: technical\n",
            "ENTITIES: {}\n",
            "NEXT STEP: Identify the specific technical issue or topic that needs to be addressed and gather relevant information or resources to assist in resolving it.\n",
            "FINAL RESPONSE:\n",
            " Thanks for reaching out! I see this looks like a **technical** request.\n",
            "\n",
            "\n",
            "**Next step:** Identify the specific technical issue or topic that needs to be addressed and gather relevant information or resources to assist in resolving it.\n",
            "\n",
            "If that sounds good, reply **YES** to proceed, or tell me what to adjust.\n",
            "---\n",
            "USER: My headphones throw error code 0x501 and won't pair.\n",
            "INTENT: technical\n",
            "ENTITIES: {}\n",
            "NEXT STEP: Identify the specific technical issue or topic that needs to be addressed and gather relevant information or resources to assist in resolving it.\n",
            "FINAL RESPONSE:\n",
            " Thanks for reaching out! I see this looks like a **technical** request.\n",
            "\n",
            "\n",
            "**Next step:** Identify the specific technical issue or topic that needs to be addressed and gather relevant information or resources to assist in resolving it.\n",
            "\n",
            "If that sounds good, reply **YES** to proceed, or tell me what to adjust.\n",
            "---\n",
            "USER: Hi, I just have a general question about your shipping times.\n",
            "INTENT: other\n",
            "ENTITIES: {}\n",
            "NEXT STEP: Since the intent is \"other\" and there are no specific entities provided, a concise next step for the agent could be: \n",
            "\n",
            "\"Ask the user for more details or clarification on their request.\"\n",
            "FINAL RESPONSE:\n",
            " Thanks for reaching out! I see this looks like a **other** request.\n",
            "\n",
            "\n",
            "        **Next step:** Since the intent is \"other\" and there are no specific entities provided, a concise next step for the agent could be: \n",
            "\n",
            "\"Ask the user for more details or clarification on their request.\"\n",
            "\n",
            "        If that sounds good, reply **YES** to proceed, or tell me what to adjust.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "@dataclass\n",
        "class CSState:\n",
        "    user_message: str\n",
        "    intent: str | None = None\n",
        "    entities: dict = field(default_factory=dict)\n",
        "    next_step: str | None = None\n",
        "    final_response: str | None = None\n",
        "\n",
        "class CustomerSupportChain:\n",
        "    def __init__(self, use_simulator: bool = True):\n",
        "        self.use_sim = use_simulator\n",
        "\n",
        "    def _ask_llm(self, prompt: str) -> str:\n",
        "        if self.use_sim:\n",
        "            return simulate_llm(prompt)\n",
        "        return call_llm(prompt)\n",
        "\n",
        "    def classify_intent(self, state: CSState) -> None:\n",
        "        prompt = f\"\"\"\n",
        "        Classify intent (billing | account | technical | other) for this user message.\n",
        "        Return just the label.\n",
        "        USER: {state.user_message}\n",
        "        (Instruction: Classify intent)\n",
        "        \"\"\"\n",
        "        state.intent = self._ask_llm(prompt).strip()\n",
        "\n",
        "    def extract_entities(self, state: CSState) -> None:\n",
        "        prompt = f\"\"\"\n",
        "        Extract the key entities from the message: order IDs, error codes, product names.\n",
        "        USER: {state.user_message}\n",
        "        Intent: {state.intent}\n",
        "        \"\"\"\n",
        "        raw = self._ask_llm(prompt)\n",
        "        # quick parse for our simulator's \"key=value\" style\n",
        "        entities = {}\n",
        "        for part in raw.split(\",\"):\n",
        "            if \"=\" in part:\n",
        "                k, v = part.split(\"=\", 1)\n",
        "                entities[k.strip()] = v.strip()\n",
        "        state.entities = entities\n",
        "\n",
        "    def propose_next_step(self, state: CSState) -> None:\n",
        "        prompt = f\"\"\"\n",
        "        Based on the intent \"{state.intent}\" and entities {state.entities},\n",
        "        propose a concise next step for the agent.\n",
        "        \"\"\"\n",
        "        state.next_step = self._ask_llm(prompt).strip()\n",
        "\n",
        "    def craft_final_response(self, state: CSState) -> None:\n",
        "        # A simple, friendly template\n",
        "        state.final_response = dedent(f\"\"\"\n",
        "        Thanks for reaching out! I see this looks like a **{state.intent}** request.\n",
        "        {('I noted these details: ' + str(state.entities)) if state.entities else ''}\n",
        "\n",
        "        **Next step:** {state.next_step}\n",
        "\n",
        "        If that sounds good, reply **YES** to proceed, or tell me what to adjust.\n",
        "        \"\"\").strip()\n",
        "\n",
        "    def run(self, user_message: str) -> CSState:\n",
        "        state = CSState(user_message=user_message)\n",
        "        self.classify_intent(state)\n",
        "        self.extract_entities(state)\n",
        "        self.propose_next_step(state)\n",
        "        self.craft_final_response(state)\n",
        "        return state\n",
        "\n",
        "# Demo\n",
        "chain = CustomerSupportChain(use_simulator=False)\n",
        "\n",
        "examples = [\n",
        "    \"I was double-charged on my invoice for the Pro Plan. Order #12345.\",\n",
        "    \"I can't log in—password reset keeps failing for my Premium subscription.\",\n",
        "    \"My headphones throw error code 0x501 and won't pair.\",\n",
        "    \"Hi, I just have a general question about your shipping times.\"\n",
        "]\n",
        "\n",
        "for msg in examples:\n",
        "    result = chain.run(msg)\n",
        "    print('---')\n",
        "    print('USER:', msg)\n",
        "    print('INTENT:', result.intent)\n",
        "    print('ENTITIES:', result.entities)\n",
        "    print('NEXT STEP:', result.next_step)\n",
        "    print('FINAL RESPONSE:\\n', result.final_response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4714b434",
      "metadata": {
        "id": "4714b434"
      },
      "source": [
        "\n",
        "## 2) Code Generation with ReACT Prompting\n",
        "\n",
        "We illustrate a minimal ReACT-style loop for a coding task. The \"agent\":\n",
        "\n",
        "- Prints **Thought** → what to do next  \n",
        "- Takes an **Action** → e.g., `write_code`  \n",
        "- Gets an **Observation** → e.g., test results  \n",
        "- Repeats until tests pass or attempts are exhausted\n",
        "\n",
        "For demonstration, we solve `two_sum(nums, target)` using a hash map approach.  \n",
        "Swap `simulate_llm(...)` with `call_llm(...)` to use a real model.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textwrap import dedent\n",
        "import re\n",
        "\n",
        "def simulate_llm(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Tiny prompt-response simulator so the notebook runs without external APIs.\n",
        "    \"\"\"\n",
        "    lower = prompt.lower()\n",
        "\n",
        "    # Intent classification (very naive)\n",
        "    if \"classify intent\" in lower:\n",
        "        if any(k in lower for k in [\"charge\", \"refund\", \"invoice\", \"billing\"]):\n",
        "            return \"billing\"\n",
        "        if any(k in lower for k in [\"can't log in\", \"password\", \"login\", \"signin\", \"sign in\"]):\n",
        "            return \"account\"\n",
        "        if any(k in lower for k in [\"error\", \"bug\", \"broken\", \"doesn't work\", \"crash\", \"not working\", \"501\", \"500\", \"404\"]):\n",
        "            return \"technical\"\n",
        "        return \"other\"\n",
        "\n",
        "    # Entity extraction (toy examples)\n",
        "    if \"extract the key entities\" in lower or \"extract entities\" in lower:\n",
        "        order = re.findall(r\"(?:order|ticket)\\s*#?\\s*([a-z0-9\\-]+)\", prompt, flags=re.I)\n",
        "        error = re.findall(r\"(?:error(?:\\s*code)?\\s*[:#]?\\s*)([A-Za-z0-9x\\-]+)\", prompt, flags=re.I)\n",
        "        product = []\n",
        "        if \"pro plan\" in lower: product.append(\"Pro Plan Subscription\")\n",
        "        if \"premium\" in lower: product.append(\"Premium Subscription\")\n",
        "        if \"headphones\" in lower: product.append(\"Headphones\")\n",
        "        if not order and \"12345\" in lower: order.append(\"12345\")\n",
        "        parts = []\n",
        "        if order: parts.append(f\"order_ids={order}\")\n",
        "        if error: parts.append(f\"errors={error}\")\n",
        "        if product: parts.append(f\"products={product}\")\n",
        "        return \", \".join(parts) if parts else \"None found\"\n",
        "\n",
        "    # Resolution proposal (toy examples)\n",
        "    if \"propose a concise next step\" in lower or \"next step\" in lower:\n",
        "        if \"billing\" in lower:\n",
        "            return \"Offer a refund or billing adjustment and confirm the last 4 digits of the payment method.\"\n",
        "        if \"account\" in lower:\n",
        "            return \"Send a secure password reset link and verify email ownership.\"\n",
        "        if \"technical\" in lower:\n",
        "            return \"Ask for logs/screenshot, provide quick fixes, and offer to escalate a ticket.\"\n",
        "        return \"Ask for more details and route to the best team.\"\n",
        "\n",
        "    # ReACT-ish code generation guidance (this is the Part 1 fix)\n",
        "    if \"react\" in lower and \"write python\" in lower and \"two_sum\" in lower:\n",
        "        return dedent(\"\"\"\n",
        "        Thought: Use a hash map to find complements in O(n).\n",
        "        Action: write_code\n",
        "        Action Input:\n",
        "        ```python\n",
        "        def two_sum(nums, target):\n",
        "            seen = {}\n",
        "            for i, x in enumerate(nums):\n",
        "                comp = target - x\n",
        "                if comp in seen:\n",
        "                    return [seen[comp], i]\n",
        "                seen[x] = i\n",
        "            return None\n",
        "        ```\n",
        "        \"\"\")\n",
        "\n",
        "    # Self-reflection critique\n",
        "    if \"critique this summary\" in lower:\n",
        "        issues = []\n",
        "        if len(prompt.split()) < 80:\n",
        "            issues.append(\"Too brief; lacks key details.\")\n",
        "        if \"conclusion\" not in lower:\n",
        "            issues.append(\"No clear conclusion.\")\n",
        "        if \"evidence\" not in lower:\n",
        "            issues.append(\"Missing supporting evidence.\")\n",
        "        return \"Critique: \" + (\" \".join(issues) if issues else \"Looks solid overall.\")\n",
        "\n",
        "    # Self-reflection revision\n",
        "    if \"rewrite an improved summary\" in lower:\n",
        "        return dedent(\"\"\"\n",
        "        Improved Summary: The article explains the core problem, outlines two viable solutions with trade-offs,\n",
        "        and concludes by recommending the option with better long-term maintainability. It adds one concrete example\n",
        "        and a brief note on limitations to increase credibility.\n",
        "        \"\"\").strip()\n",
        "\n",
        "    # ✅ DEFAULT FALLBACK (prevents None from being returned)\n",
        "    return \"Thought: Falling back.\\nAction: write_code\\nAction Input:\\n```python\\n# no-op\\n```\\n\"\n"
      ],
      "metadata": {
        "id": "z_drhVfPl67x"
      },
      "id": "z_drhVfPl67x",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "825365cf",
      "metadata": {
        "id": "825365cf"
      },
      "outputs": [],
      "source": [
        "# Make sure these are imported once at top of your notebook\n",
        "from textwrap import dedent\n",
        "import re, traceback\n",
        "\n",
        "class ReactCodeAgent:\n",
        "    def __init__(self, use_simulator: bool = True, max_iters: int = 3):\n",
        "        self.use_sim = use_simulator\n",
        "        self.max_iters = max_iters\n",
        "\n",
        "    def _ask_llm(self, prompt: str) -> str:\n",
        "        resp = simulate_llm(prompt) if self.use_sim else call_llm(prompt)\n",
        "        return resp if isinstance(resp, str) else \"\"\n",
        "\n",
        "    def solve_two_sum(self):\n",
        "        problem = dedent(\"\"\"\n",
        "        System: You are a helpful coding assistant.\n",
        "        User: Use ReACT. Think step-by-step and write Python for a function:\n",
        "        - Name: two_sum(nums, target)\n",
        "        - Return the indices of two numbers such that they add up to target.\n",
        "        - If none, return None.\n",
        "        - Provide only valid Python inside a fenced code block.\n",
        "\n",
        "        Follow this format:\n",
        "        Thought: <your reasoning>\n",
        "        Action: write_code\n",
        "        Action Input:\n",
        "        ```python\n",
        "        # your code\n",
        "        ```\n",
        "        \"\"\")\n",
        "        print(\"PROMPT SENT TO MODEL (abbrev):\\n\", problem.splitlines()[0], \"...\")\n",
        "\n",
        "        ns, code = {}, \"\"\n",
        "        for i in range(1, self.max_iters + 1):\n",
        "            print(f\"\\n=== Iteration {i} ===\")\n",
        "            react = self._ask_llm(problem)\n",
        "            if not isinstance(react, str) or not react.strip():\n",
        "                print(\"Observation: Empty response from LLM. Retrying...\")\n",
        "                continue\n",
        "            print(react.strip())\n",
        "\n",
        "            # Extract code block\n",
        "            code_match = re.search(r\"```python(.*?)```\", react, flags=re.S | re.I)\n",
        "            if not code_match:\n",
        "                print(\"Observation: No code found. Asking model to try again...\")\n",
        "                continue\n",
        "\n",
        "            code = code_match.group(1).strip()\n",
        "            print(\"Observation: Received code. Executing tests...\")\n",
        "            try:\n",
        "                ns = safe_exec(code)\n",
        "                tests = [\n",
        "                    (\"two_sum([2,7,11,15], 9)\", [0,1]),\n",
        "                    (\"two_sum([3,2,4], 6)\", [1,2]),\n",
        "                    (\"two_sum([3,3], 6)\", [0,1]),\n",
        "                    (\"two_sum([1,2,3], 10)\", None),\n",
        "                ]\n",
        "                results = run_tests(ns, tests)\n",
        "                for line in results:\n",
        "                    print(line)\n",
        "                if all(\"PASS\" in line for line in results):\n",
        "                    print(\"\\nFinal Answer: All tests passed ✅\")\n",
        "                    return ns, code\n",
        "                else:\n",
        "                    print(\"Thought: Some tests failed. I should revise the code.\")\n",
        "            except Exception:\n",
        "                print(\"Observation: Execution error:\\n\", traceback.format_exc())\n",
        "                print(\"Thought: Execution failed. I should fix syntax/logic and retry.\")\n",
        "\n",
        "        print(\"\\nFinal Answer: Reached max iterations. Returning last attempt.\")\n",
        "        return ns, code\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9c01515",
      "metadata": {
        "id": "a9c01515"
      },
      "source": [
        "\n",
        "## 3) Self-Reflection Prompt for Improving Output\n",
        "\n",
        "We take a **draft summary**, ask for a **critique**, and then request a **revision**.  \n",
        "In a real setup you'd call your LLM twice; here we simulate both steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(simulate_llm(\"critique this summary: test\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSlz6-ALnmPD",
        "outputId": "a192cc5b-70c5-4571-d947-cb52a4f605ef"
      },
      "id": "jSlz6-ALnmPD",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: Falling back.\n",
            "Action: write_code\n",
            "Action Input:\n",
            "```python\n",
            "# no-op\n",
            "```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect, textwrap\n",
        "src = inspect.getsource(simulate_llm)\n",
        "print(textwrap.dedent(src))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9aF8SvSoBJI",
        "outputId": "dc3a3e52-237a-42c0-e645-7985bec52cee"
      },
      "id": "x9aF8SvSoBJI",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def simulate_llm(prompt: str) -> str:\n",
            "    lower = prompt.lower()\n",
            "\n",
            "    # ... your other branches (intent/entity/next-step) ...\n",
            "\n",
            "    # ReACT-ish code generation guidance (ensure this exact trigger)\n",
            "    if \"react\" in lower and \"write python\" in lower and \"two_sum\" in lower:\n",
            "        return dedent(\"\"\"\n",
            "        Thought: Use a hash map to find complements in O(n).\n",
            "        Action: write_code\n",
            "        Action Input:\n",
            "        ```python\n",
            "        def two_sum(nums, target):\n",
            "            seen = {}\n",
            "            for i, x in enumerate(nums):\n",
            "                comp = target - x\n",
            "                if comp in seen:\n",
            "                    return [seen[comp], i]\n",
            "                seen[x] = i\n",
            "            return None\n",
            "        ```\n",
            "        \"\"\")\n",
            "    # ⬇️ DEFAULT FALLBACK to avoid returning None\n",
            "    return \"Thought: Falling back.\\nAction: write_code\\nAction Input:\\n```python\\n# no-op\\n```\\n\"\n",
            "\n",
            "    def __init__(self, use_simulator: bool = True, max_iters: int = 3):\n",
            "        self.use_sim = use_simulator\n",
            "        self.max_iters = max_iters\n",
            "\n",
            "    def _ask_llm(self, prompt: str) -> str:\n",
            "        if self.use_sim:\n",
            "            return simulate_llm(prompt)\n",
            "        return call_llm(prompt)\n",
            "\n",
            "    def solve_two_sum(self):\n",
            "        problem = dedent(\"\"\"\n",
            "        System: You are a helpful coding assistant.\n",
            "        User: Use ReACT. Think step-by-step and write Python for a function:\n",
            "        - Name: two_sum(nums, target)\n",
            "        - Return the indices of two numbers such that they add up to target.\n",
            "        - If none, return None.\n",
            "        - Provide only valid Python inside a fenced code block.\n",
            "\n",
            "        Follow this format:\n",
            "        Thought: <your reasoning>\n",
            "        Action: write_code\n",
            "        Action Input:\n",
            "        ```python\n",
            "        # your code\n",
            "        ```\n",
            "        \"\"\")\n",
            "        print(\"PROMPT SENT TO MODEL (abbrev):\\n\", problem.splitlines()[0], \"...\")\n",
            "\n",
            "        for i in range(1, self.max_iters + 1):\n",
            "            print(f\"\\n=== Iteration {i} ===\")\n",
            "            react = self._ask_llm(problem)\n",
            "            print(react.strip())\n",
            "\n",
            "            # Extract code block\n",
            "            code_match = re.search(r\"```python(.*?)```\", react, flags=re.S | re.I)\n",
            "            if not code_match:\n",
            "                print(\"Observation: No code found. Asking model to try again...\")\n",
            "                continue\n",
            "\n",
            "            code = code_match.group(1).strip()\n",
            "            print(\"Observation: Received code. Executing tests...\")\n",
            "            try:\n",
            "                ns = safe_exec(code)\n",
            "                tests = [\n",
            "                    (\"two_sum([2,7,11,15], 9)\", [0,1]),\n",
            "                    (\"two_sum([3,2,4], 6)\", [1,2]),\n",
            "                    (\"two_sum([3,3], 6)\", [0,1]),\n",
            "                    (\"two_sum([1,2,3], 10)\", None),\n",
            "                ]\n",
            "                results = run_tests(ns, tests)\n",
            "                for line in results:\n",
            "                    print(line)\n",
            "                if all(\"PASS\" in line for line in results):\n",
            "                    print(\"\\nFinal Answer: All tests passed ✅\")\n",
            "                    return ns, code\n",
            "                else:\n",
            "                    print(\"Thought: Some tests failed. I should revise the code.\")\n",
            "            except Exception:\n",
            "                print(\"Observation: Execution error:\\n\", traceback.format_exc())\n",
            "                print(\"Thought: Execution failed. I should fix syntax/logic and retry.\")\n",
            "\n",
            "        print(\"\\nFinal Answer: Reached max iterations. Returning last attempt.\")\n",
            "        return ns if 'ns' in locals() else {}, code if 'code' in locals() else \"\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "4ae29d40",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ae29d40",
        "outputId": "73df09a8-8320-4926-d122-2b6a864fd13a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Critique ===\n",
            " Thought: Falling back.\n",
            "Action: write_code\n",
            "Action Input:\n",
            "```python\n",
            "# no-op\n",
            "``` \n",
            "\n",
            "=== Improved Summary ===\n",
            " Thought: Falling back.\n",
            "Action: write_code\n",
            "Action Input:\n",
            "```python\n",
            "# no-op\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def reflective_improvement(draft_summary: str) -> tuple[str, str]:\n",
        "    critique_prompt = f\"\"\"\n",
        "    Critique this summary for clarity, completeness, evidence, and conclusion.\n",
        "    Be specific and concise. Then list 2-3 concrete improvements.\n",
        "\n",
        "    SUMMARY:\n",
        "    {draft_summary}\n",
        "    \"\"\"\n",
        "    critique = simulate_llm(critique_prompt)\n",
        "\n",
        "    improve_prompt = f\"\"\"\n",
        "    Rewrite an improved summary that addresses the critique below.\n",
        "    Keep it 4-6 sentences, specific, and self-contained.\n",
        "\n",
        "    CRITIQUE:\n",
        "    {critique}\n",
        "    SUMMARY TO IMPROVE:\n",
        "    {draft_summary}\n",
        "    \"\"\"\n",
        "    improved = simulate_llm(improve_prompt)\n",
        "    return critique.strip(), improved.strip()\n",
        "\n",
        "# Demo\n",
        "draft = (\n",
        "    \"The article talks about a new approach to system design. \"\n",
        "    \"It briefly mentions trade-offs but doesn't go into much detail. \"\n",
        "    \"There's no real conclusion.\"\n",
        ")\n",
        "\n",
        "critique, improved = reflective_improvement(draft)\n",
        "print(\"=== Critique ===\\n\", critique, \"\\n\")\n",
        "print(\"=== Improved Summary ===\\n\", improved)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "525a160a",
      "metadata": {
        "id": "525a160a"
      },
      "source": [
        "\n",
        "### Notes & Next Steps\n",
        "\n",
        "- To use a real model, implement `call_llm(...)` and switch `use_simulator=False` in the demos.\n",
        "- You can extend the prompt chain with more steps (e.g., escalation, confirmation).\n",
        "- The ReACT loop can be generalized to arbitrary coding tasks and richer toolsets.\n",
        "- The reflection step is easy to wrap into any generation workflow as a second pass.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}